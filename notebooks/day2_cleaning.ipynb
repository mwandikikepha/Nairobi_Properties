{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccf9a52",
   "metadata": {},
   "source": [
    "Day2.ipynb - We already have 800+ rows. Cleaning this data into useful insights will happen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087704d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9c05d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rows: 808\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/kepha/nairobi_property/data/raw_listings.csv\")\n",
    "df_clean = df.copy()\n",
    "print(f\"Starting rows: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0872cfb",
   "metadata": {},
   "source": [
    "REMOVING DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860ec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates before: 432\n"
     ]
    }
   ],
   "source": [
    "print(f\"Duplicates before: {df_clean.duplicated().sum()}\")\n",
    "df_clean = df_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5865e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing duplicates: 376\n",
      "Duplicates after: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows after removing duplicates: {len(df_clean)}\")\n",
    "print(f\"Duplicates after: {df_clean.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715905c5",
   "metadata": {},
   "source": [
    "REMOVING PRICE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f58a00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After price filtering, 1M-500M: 375 rows remain\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean[(df_clean['price_kes'] >= 1_000_000) & \n",
    "                    (df_clean['price_kes'] <= 500_000_000)]\n",
    "print(f\"After price filtering, 1M-500M: {len(df_clean)} rows remain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0303e",
   "metadata": {},
   "source": [
    "SORTING SIZE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "110c7be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       352.000000\n",
      "mean       6340.813750\n",
      "std       18488.097679\n",
      "min          96.880000\n",
      "25%         839.590000\n",
      "50%        1291.680000\n",
      "75%        2454.190000\n",
      "max      129170.000000\n",
      "Name: size_sqft, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean['size_sqft'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47997a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying suspiciously small sizes, under 200 sqft but the price is over 10M KES\n",
    "suspicious_small = (df_clean['size_sqft'] < 200) & (df_clean['price_kes'] > 10000000)\n",
    "print(df_clean.loc[suspicious_small, ['location', 'bedrooms', 'size_sqft', 'price_kes']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b042ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiply the size by 10 for these suspicious entries, assuming a possible decimal point error\n",
    "df_clean.loc[suspicious_small, 'size_sqft'] = df_clean.loc[suspicious_small, 'size_sqft'] * 10\n",
    "print(df_clean.loc[suspicious_small, ['location', 'bedrooms', 'size_sqft', 'price_kes']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaacd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add acres column to understand scale\n",
    "df_clean['size_acres'] = df_clean['size_sqft'] / 43560\n",
    "\n",
    "large_properties = df_clean[df_clean['size_sqft'] > 20000][\n",
    "    ['location', 'bedrooms', 'size_sqft', 'size_acres', 'price_kes']\n",
    "].sort_values('size_sqft', ascending=False)\n",
    "\n",
    "print(large_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83f219bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have decided to remove the large properties that we multiplied by 10, as they are likely outliers and may skew the analysis. I will filter them out based on their size and price per sqft.\n",
    "large_properties = df_clean[df_clean['size_sqft'] > 20000]\n",
    "df_clean = df_clean[~df_clean.index.isin(large_properties.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d06e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After size filtering (>200 sqft): 351 rows\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean[(df_clean['size_sqft'] >= 200) | (df_clean['size_sqft'].isna())]\n",
    "print(f\"After size filtering (>200 sqft): {len(df_clean)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2dcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing sizes: 23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_sizes = df_clean['size_sqft'].isna().sum()\n",
    "print(f\"Rows with missing sizes: {missing_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbaab7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing sizes: 328 rows\n"
     ]
    }
   ],
   "source": [
    "#DRopping rows with missing sizes for now, as we cannot impute them without more information\n",
    "df_clean = df_clean.dropna(subset=['size_sqft'])\n",
    "print(f\"After dropping missing sizes: {len(df_clean)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea036880",
   "metadata": {},
   "source": [
    "SORTING AMMENITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c0f8bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing amenities: 325 rows\n"
     ]
    }
   ],
   "source": [
    "#dropping properties with no ammenities listed, as they are likely incomplete listings\n",
    "df_clean = df_clean.dropna(subset=['amenities'])\n",
    "print(f\"After dropping missing amenities: {len(df_clean)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1f52d",
   "metadata": {},
   "source": [
    "FINALLY : STANDARDIZING THE TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "332ff7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['location'] = df_clean['location'].str.title().str.strip()\n",
    "df_clean['property_type'] = df_clean['property_type'].str.title().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1325c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete, Kept 325 listings\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cleaning complete, Kept {len(df_clean)} listings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590eca92",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e178db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: price_per_sqft\n"
     ]
    }
   ],
   "source": [
    "# Price per square foot\n",
    "df_clean['price_per_sqft'] = df_clean['price_kes'] / df_clean['size_sqft']\n",
    "print(\"Created: price_per_sqft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "424e42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: amenity_score\n"
     ]
    }
   ],
   "source": [
    "#Creating amenity score\n",
    "#Counts amenities each property has by counting commas eg \"Parking, Pool, Gym\" has 2 commas = 3 amenities.\n",
    "\n",
    "def count_amenities(amenities_str):\n",
    "    if pd.isna(amenities_str) or amenities_str == 'None':\n",
    "        return 0\n",
    "    return amenities_str.count(',') + 1\n",
    "\n",
    "df_clean['amenity_score'] = df_clean['amenities'].apply(count_amenities)\n",
    "print(\"Created: amenity_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "259c24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: month\n"
     ]
    }
   ],
   "source": [
    "# Month from listing date\n",
    "df_clean['listing_date'] = pd.to_datetime(df_clean['listing_date'])\n",
    "df_clean['month'] = df_clean['listing_date'].dt.month\n",
    "print(\"Created: month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dd9c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(columns=['size_acres'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae4db4",
   "metadata": {},
   "source": [
    "SAVE TO CLEAN_LISTINGS CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80d08250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"/home/kepha/nairobi_property/data/clean_listings.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
